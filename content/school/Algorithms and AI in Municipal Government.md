---
title: Lenses to Consider Current & Future Uses of Algorithmic Tools & Artificial Intelligence in Municipal Government
created: 2025-12-12
---
> Samuel Eaton, Kristy Ketcham, & Jake Brymner
> University of California Berkeley
> Public Policy 290: Case Studies in Prediction, Public Policy, and AI
> Avi Feller and Deborah Raji

# Abstract

Local governments are increasingly using artificial intelligence (AI) to support decisions in areas such as inspections, permitting, and service delivery, posing significant consequences for program outcomes and equity. Drawing on interdisciplinary literature, this paper proposes a framework for evaluating AI use in local government practice that accounts for institutional constraints such as procurement, legal authority, and legacy systems. It highlights four key considerations: data representation, fairness metrics, accountability, and implementation, with particular attention to how tools interact with bureaucratic discretion. This framework is then applied to two case studies, a fire risk prediction model in Washington, DC and an AI-assisted permitting tool in Los Angeles County. The paper concludes that responsible AI use in local government requires aligning technical design choices with program goals, constituent needs, and existing governance structures rather than relying on standardized technical solutions.

# Background

Algorithms, and now AI systems, play an increasingly important role in government and society. They are given an immense amount of power over human life through their uses in impactful domains like welfare eligibility, fraud detection, aid targeting, hiring, and healthcare. Politically, they create a new class of “coding elite” and intensify actuarial logics that perpetuate inequality and reorganize social behavior for algorithmic means (Burrell & Fourcade, 2021). They also can act toward political ends as technologies are both used to settle political matters and may be strongly compatible with certain political arrangements (Winner, 2020). The two dominant approaches to AI in government are viewing it as a product or service that is procured for government use, and as a policy-making endeavor. Local governments  must both adapt their procurement process to new AI products and implement AI policy or risk ceding decision-making power to algorithm designers. It is important to shift from a solely procurement mindset to a policymaking mindset when making system design choices, as “policy choices embedded in machine-learning system design fail the prohibition against arbitrary and capricious agency actions” (Mulligan & Bamberger, 2019). In order to deploy safe AI systems, local governments need to take a holistic approach—buildling them into the preexisting informal and formal institutional environments and honoring the context of use; technical design choices alone are not enough (Dobbe, 2022). While increased volume and range of AI tools available for local government use presents new opportunities to reimagine public sector service delivery, policymakers must consider the broader socio-technical context of their agency before proceeding.

## Context of Municipal Governments & Artificial Intelligence Tools

Local municipal governments and their agencies operate within a unique context that is highly relevant for AI deployment. Local governments create “public value” for their constituents when aligning organizational capacity with the desires of its authorizing environment (Moore, 1995). The authorizing environment of local government is unique in its dual nature: it is close to its residents and highly trusted, but often limited in its authority to solve problems or deliver value. Local governments in the U.S. derive their legal authority from state governments (National League of Cities, 2016). This legal foundation of municipal government constrains authority around even locally held decisions within larger state (or federal) statutes. In other cases, local governments may utilize funding from state or federal government programs that entail specific restrictions on how funds might be utilized or drive local decisionmaking based on the fiscal incentives they create (e.g., federal “continuum of care” funding to reduce homelessness, U.S. Department of Transportation grants). While local policymakers may be held responsible for solving local problems, they often do not have the ability to decide upon the tools available with which to address them. 

There are other unique political features to local government that are important to considering its organizational and societal settings. Survey data consistently finds that local government is the most widely trusted level of government on a bipartisan basis, which has only grown more significant as polarization has reduced trust in state and federal governments (Jones, 2025). Residents are nearer to local government policymakers than those at the state or federal level, its deliberations are more accessible, and the matters with which local government is concerned are typically more comprehensible to lay audiences than those at the state or federal level (Nabatchi & Blomgren Amsler, 2014). Local governments are also the most highly unionized sector of the U.S. economy per Bureau of Labor Statistics data, which found that 38% of all workers in local government were unionized (U.S. Bureau of Labor Statistics, 2025). Collective bargaining agreements may restrict how duties for roles within local government can be amended to reflect new responsibilities involving the creation, use, or maintenance of algorithmic tools, limiting the pace of adoption and innovation (Goldsmith & Yang, 2024). The combination of high public trust and robust union representation situates local government as a venue ripe for collaborative design of AI tools but also fraught with tension points should a tool be hastily or unilaterally implemented. 

From a technical standpoint, local governments face the same challenges as many other public sector entities. Government procurement processes are notoriously inefficient. Public sector contracting processes are often overly complex, task oriented, and biased toward incumbent vendors with prior or existing government contracts that can withstand the lengthy journey to entering an agreement (Burger et al., 2025). While many forms of AI are not new or already being used in local government, more recent iterations of generative AI are still relatively unfamiliar to many working in local government according to the National League of Cities (NLC). Given the tendency of procurement processes to default to prior contract templates and means for setting vendor tasks, this suggests that local governments may face significant challenges in adapting to soliciting impactful contract relationships with those best suited to deliver high-quality AI products and services. (National League of Cities, 2024)

The NLC convened an advisory committee and partnered with Google to explore the key opportunities and risks associated with AI among its members. Many of the barriers cited were of a political nature (e.g., organizational culture changes, sustainability of political support), while others were technical (e.g., data literacy and readiness, data security) or resource-driven (e.g., funding, access to necessary infrastructure) (National League of Cities, 2024). In 2023, the City of San Jose convened the Government AI Coalition (Gov/AI Coalition) that brings together local and state agencies utilizing or exploring AI tools. The Gov/AI Coalition has published resources that encourage local government officials contemplating the use of AI to reflect on similar questions around data security and community engagement, among others on the technical components of models, but less directly address other organizational considerations. (Government AI Coalition & Center for Scientific Evidence in Public Issues). 

## Data Availability, Bias, & Fairness

Understanding the potential biases in government data requires attention to three central issues: sampling error, non-sampling error, and structural under-representation. These issues shape what models can learn and directly influence their fairness once deployed (Feng & Wu, 2019) .

Issues of sampling, recording, and coverage shape how well administrative data reflects the populations that local governments aim to serve. Sampling error arises when datasets capture only a subset of residents, producing distorted averages and leading models to learn misleading patterns. This is common in municipal settings, where data such as 311 calls or police reports reflect who engages with public systems rather than the true distribution of need across neighborhoods.

Even when sampling is unbiased, non-sampling error can distort what is recorded. Miscoded fields, inconsistent formats, and missing information introduce inaccuracies that models interpret as true signals. These problems are embedded in the administrative processes that produce public-sector data. Administrative datasets are shaped by local workflows, organizational priorities, and the idiosyncrasies of workers entering information under pressure (Sendak, 2025). Research on human services shows similar patterns, noting that caseworkers often document only what they have time and capacity to record, which leads to datasets that reflect staff workload and discretion more than household circumstances (Xia et al., 2022). When models learn from these kinds of inputs, they internalize the biases of bureaucratic workflow rather than the conditions of residents.

A more fundamental concern is structural under-representation, which occurs when entire groups rarely appear in the data at all. Even very large datasets can have major gaps. For example, widely used computer vision datasets draw disproportionately from the United States and Western Europe, with minimal representation of images from South Asia, Africa, or Latin America, which produces systematic performance failures on underrepresented groups (Yucer et al., 2025). Local governments face parallel blind spots. Residents who are unhoused, who live in informal or overcrowded housing, or who avoid government institutions altogether appear only sporadically in municipal records. Studies of child welfare prediction systems show that algorithms trained on incomplete or unevenly collected administrative data systematically misestimate risk for structurally underrepresented families, reinforcing existing inequalities rather than addressing them (Chouldechova et al., 2018). Models trained on these datasets cannot reliably allocate resources or identify needs in communities they do not represent.

Another consideration to highlight is data drift, which can undermine model performance once a system is deployed. Data drift occurs when the relationship between inputs and outcomes shifts because documentation practices, policies, staffing patterns, or population characteristics change over time. Research in clinical prediction modeling demonstrates how vulnerable algorithms are to these shifts. Van Calster and colleagues show that models which initially performed well deteriorated substantially after hospitals adopted new coding standards or altered how information was recorded (Ross, 2022). Local governments face similar risks as agencies routinely update intake forms, revise eligibility criteria, or modify case-management systems. Without continuous monitoring and recalibration, models can become inaccurate or inequitable even while appearing to function normally within government workflows.

## Fairness Metrics 

Fairness metrics help policymakers define what fairness means in a particular application. No metric is universally appropriate. Each reflects a different normative priority, creates different tradeoffs, and must be selected based on the goals of the program where the model will be used. Below we discuss three useful fairness metrics.

*Statistical parity* requires that groups receive positive predictions at equal rates. This metric can be useful when policymakers want to distribute benefits or preventive services evenly across communities. However, when groups face different underlying conditions, enforcing equal rates can require treating individuals with different levels of risk as if they were the same, which may reduce accuracy and undermine policy goals (Corbett-Davies et al., 2017). 

*Conditional statistical parity* allows differences between groups only when these differences can be explained by legitimate factors. The challenge lies in determining which variables are legitimate. Research shows that many administrative variables commonly treated as neutral, such as prior contact with government systems, actually reflect structural inequities and can reproduce disparities when used in models (Xiang & Raji, 2019). Conditional parity can be useful when a jurisdiction has clear eligibility criteria, but those criteria must be reviewed to ensure they do not reinforce existing biases.

*Predictive equality* requires equal false-positive rates across groups. A false positive occurs when the model predicts that a condition or risk is present when it is not. The consequences depend on the domain. In public safety or child welfare, a false positive can trigger unnecessary investigation or monitoring. In service delivery, it may misallocate resources that could have been better used elsewhere. Predictive equality is most relevant when policymakers aim to reduce harmful or costly false alarms (Corbett-Davies et al., 2017).

The impossibility result explains why these fairness metrics often conflict. Scholars show that when groups have different underlying base rates, it is mathematically impossible for a model to satisfy statistical parity, equal error rates, and calibration at the same time (Corbett-Davies et al., 2017). The debate around the COMPAS pretrial risk assessment makes this concrete. COMPAS developers evaluated the tool using calibration, meaning that individuals with the same score had the same likelihood of reoffending regardless of race. ProPublica applied a different metric and found that Black defendants were incorrectly labeled high-risk at higher rates than white defendants (Larson et al., 2016). Both evaluations were correct, but each relied on a different fairness criterion, and COMPAS could not satisfy both simultaneously due to the impossibility result (Corbett-Davies et al., 2016).

When selecting a fairness metric, local governments should begin by defining the goal of the program and identifying which errors create the greatest harm for individuals and groups. In practice, cities must weigh whether a false positive would inappropriately trigger an investigation, delay a permit, or divert staff toward low-risk cases, versus whether a false negative would mean missing unsafe housing conditions, unmet social service needs, or emerging public health risks. These choices are inherently local: demographic composition and policy priorities vary widely across jurisdictions, so a fairness definition that is appropriate in one city may produce very different results in another. Thinking carefully about program goals, error impacts, and equity across communities helps ensure that the chosen fairness metric reflects real policy priorities.

## Accountability & Liability

It is important to ground this discussion of accountability and liability in new AI systems with a discussion of much older problems.AI systems are algorithms themselves and inherit many of the same problems described by Eubanks. Zooming out even further, many of the same accountability problems inherent in software remain present in AI systems, albeit in different forms. In Computing and Accountability (1995), Nissenbaum provides  a framework for accountability in software that, with just a few updates, remains applicable to AI systems. She outlines four barriers to accountability, the first being the problem of many hands: since software is often a collective project, accountability is diffused. The same is true of AI, but with additional layers of complexity. Not only are AI models developed by many people, they are often built within opaque, privately held corporations for which “techlash” has largely reoriented public perception of their intervention in government (Helles & Lomborg, 2024). For large models, training data often comes from outside the organization rendering verification of its quality or origin impossible. The second barrier is bugs. When bugs are cast as inevitable it seems unreasonable to hold developers accountable. If perceived inevitability is defined as the  problem, it can also encapsulate incorrect predictions, classifications, and hallucinations as well. The third barrier is blaming the computer, which often “serves as a stopgap for something elusive, the one who is, or should be, accountable” (Nissembaum, 1995, p. 78). To complicate matters, humans have the tendency to anthropomorphize AI systems and as something that can be blamed. The last barrier to accountability is how software ownership is separate from liability. Often software vendors demand maximum property protections with minimum liability. This trend has continued to AI companies, with additional consideration that off-label use can further distance the vendor from any liability or accountability. This is problematic when the discourse around and advertisement of AI misrepresents its actual capabilities (Bender & Hanna, 2025).

Accountability is an important consideration, but not an insurmountable barrier to local governments seeking to utilize AI tools. Nissenbaum offers three recommendations toward effective accountability that we consider in conversation with suggestions from the Gov/AI CoalitionI. First, accountability should be distinct from liability. Where liability centers the experience of the victim, accountability focuses on action. When the two are vaguely combined, compensation for damages obscures the need for substantive change. While large AI developers may easily diffuse damages, assuming responsibility for repairing ineffective systems is both more difficult and impactful. While the Gov/AI Coalition references both accountability and liability, it focuses on the agency and not the vendor. The second recommendation is to strongly promote a standard of care. The Gov/AI Coalition provides an AI model factsheet (n.d.) that directly addresses this, listing model biases, audits, data protection, optimal/poor model conditions, ongoing monitoring, and other best practices. However, there is a lacking systemic nature to implementation of these practices as this fact sheet is only suggested for use in solicitation processes. The last recommendation is to enforce strict liability for software with great impact. While current litigation around AI products and claimed damages is testing the applicability of strict liability principles, courts have generally not treated software as products held to the standard in the years since Nissenbaum published this recommendation (Bosman et al., 2025) The Gov/AI Coalition’s template vendor agreement (Addendum [X]: Requirements for AI Systems, n.d.) follows suit, as there is a remediation section tending to accountability, but “liability risk” again is confined to a small “AI Incident response” section. Absent federal or state standards for liability for AI products, it becomes even more critical that local governments consider this in their procurement and contracting processes. 

While Nissenbaum’s recommendations largely fall within the realm of professional accountability (i.e., implementing standards that align bureaucrats with public values), Goldsmith and Yang (2024) also outline the ways in which AI systems change political accountability and participatory accountability in local government. For political accountability (i.e.,relationships between street level bureaucrats and supervisors), AI systems strengthen managerial control and top-down authority when they act on government employee data. This has important implications for the discretion of street-level bureaucrats. Discretion is a key means by which bureaucrats adapt to specific contexts that they encounter, and excessive control or oversight can actually hamper street-level bureaucrats' ability to perform their duties and exercise discretion (Hupe & Hill, 2007). Therefore, it is important to consider how an AI system’s use will shift power within government and shift the complex interaction of discretion and accountability. For participatory accountability (i.e.,feedback and collaboration with citizens and customers) they outline how LLMs have the potential to increase information flows between citizens and bureaucrats. However, a more important consideration is the affected community’s participation in the model itself. A recent paper on foundation models outline how they may be structurally incompatible with community participation, and at the very least require additional layers of community control to ensure equity (Suresh et al., 2024). Local governments must keep this in mind when they choose to use AI systems that have a great impact on people.

## Implementation & Practicalities of Deployment

Adoption of any new technology depends not only on its deployment, but how its use is made manifest within an organization. Most current uses of machine learning, including in public sector settings, organizations, reflect statistical techniques that are decades old relative to more recently derived mechanisms, like random forests or transformers. Organizations are slow to adopt innovations in such models due to limitations in testing of their safety outcomes relative to real-world scenarios. Public sector organizations will be even more apprehensive about deployment of more novel models as failure can undermine the credibility and trust that are highly salient concerns for government. (Narayanan & Kapoor, 2025)  

An ethnographic study by Saxena and Guha of how a state agency utilized algorithmic tools in Wisconsin provides important lessons for how other government agencies should consider potential deployments. Public sector workers are generally not trained in statistical thinking, but might be in positions that require their collecting or submitting data, interpreting algorithmic outputs, or otherwise interacting with an AI tool. Additionally, as suggested in the context section, “algorithmic decisions in the public sector must be made within the bounds of policies,

current practice, and organizational constraints.” (Saxena & Guha, 2024) While a public problem or concern might lend itself to consideration as a “prediction problem” in which an AI tool is helpful to prioritizing resources, targeting an intervention, or providing an approval, other constraints might inhibit how a public sector organization utilizes resources to act upon such a prediction (Kleinberg et al., 2015). Saxena and Guha also found that street-level bureaucrats at the agency expressed a loss of their discretion and believed that the mandated algorithmic tools did not reflect the underlying concepts of their program model. An important conclusion of the study is that any algorithmic tools must be informed by how the street-level bureaucrats tasked with utilizing them understand their work, in addition to more commonly cited criteria of transparency and interpretability for such models (Saxena & Guha, 2024). 

Discretion is employed differently by street-level bureaucrats than by AI models. Alkhatib & Bernstein (2019) point out that street-level bureaucrats exercise discretion before making their decision, adapting to unique contexts they may encounter. Algorithms only employ discretion after the decision has been made and labeled as right or wrong when this new data point is used to retrain the model, though this does not always happen in practice. The relationship between street level bureaucrats and the people they impact is also much different than that between the developer and the systems they create. This complicates how, or if, AI systems adapt to novel situations as models are not retrained (with more tailored data) for every scenario they face. 

The nature of the population impacted by decisions or actions guided by algorithmic tools also matters for how a local public sector organization considers implementation. Inequities within a population served by the local government, whether in resources, experience, political voice, or knowledge about how algorithmic tools function, will shape how various sub-populations interact with the public sector organization’s actions or decisions (Goldsmith & Yang, 2024). Sophisticated or experienced parties may seek to “game” the outcomes of a model, whether by understanding its inputs (as occurred with the algorithms studied in Wisconsin) or the means to appeal its outcomes. For example, the Assessor’s Office in Cook County, Illinois utilized an algorithmic model to more accurately and transparently assess property tax values, but ultimately the appeals process for valuations produced racial bias and a greater tax burden on lower-income residents than corporate or more affluent property owners  (Ross, 2019; Cook County Treasurer’s Office, 2025). Local governments must consider existing distributions of resources, access, and political participation and interrogate how an AI model might curtail or compound them. 

# Case Studies 

## Prediction model to identify at-risk buildings for fire inspections in Washington, DC

This case examines whether a machine learning model could help DC Fire and EMS target discretionary fire inspections more effectively. The model produced a risk score for roughly twenty thousand commercial buildings based on historical fire incidents and building characteristics from 2016 through 2023. To test its impact, the department ran a ten week randomized trial in fall 2024 in which inspectors were assigned each week to receive either a list of buildings drawn from the highest risk tier or a list of randomly selected buildings. Inspectors were not told which type of list they received. This design allowed the department to measure whether model generated guidance led to inspectors finding more serious fire causing violations. Using our framework we evaluate the strengths and limitations of the model and the pilot across data, fairness, accountability, and implementation.

### Data

The model benefits from a large and detailed dataset. It draws on information from 2016 through 2023 that includes building characteristics, permit activity, and recorded fire incidents for about twenty thousand commercial buildings. However there are gaps that limit the representativeness of the data. Inspectors cannot enter private units in multi-unit buildings, so many common hazards inside kitchens and living areas never appear in the system. Inspectors also fix dangerous conditions during the visit and often do not record them, which means that violations are missing from the data. Some buildings also have sparse records because they have limited interactions with permitting or inspection systems. These issues reflect structural limits in documentation and data collection that raise concerns about representativeness and non-sampling error in our framework. 

### Fairness

The model is designed to predict fires, so it focuses on minimizing false negatives. Missing a risky building can create serious harm, so this priority makes sense for fire prevention. A false positive means inspectors spend time on a building that does not have a violation. This is inefficient but less harmful than a false negative. The study does not examine whether these errors occur more often in certain neighborhoods, building types, or demographic groups. It also does not consider whether some buildings face greater challenges in maintaining safety and might benefit from more attention. Without a clear fairness goal or evaluation of such data it is difficult to understand how the model’s errors relate to equity in this program.

### Accountability

The case study does not clearly define who is accountable for the decisions produced by the model or for the outcomes of using it in practice. Inspectors received lists as recommendations and were encouraged to retain their own judgment. This protects discretion but also makes responsibility unclear if the model ranks a building too low or directs attention away from a building that later experiences a fire. The study shows that a randomized trial was used before citywide adoption, which is an important accountability practice because it tests whether the tool performs as intended in real conditions. Beyond this, the case does not outline any process for handling errors, updating the model, or auditing its performance over time. It is not clear who owns the model after deployment or how inaccurate or biased outputs would be corrected. 

### Implementation

The program used a sophisticated implementation strategy by testing the tool through a ten week randomized trial that showed the outcomes of inspectors using the lists and informed improvements to the process. The trial revealed that the narrow highest risk tier was not the most effective threshold and that violation identification improved when the risk tier covered a wider set of buildings. It also raised questions about how inspectors interpret and use the lists in daily work and highlighted potential blind spots in the inspection process more broadly.

## Pilot use of AI to expedite permit reviews in Los Angeles County after Eaton & Palisades Fires

In April, Governor Newsom and local Los Angeles City and County leaders announced a public-private partnership to utilize a new AI tool to expedite building permits for construction following the Eaton and Palisades Fires (Office of the California Governor, 2025). With philanthropic funding support and policymaker commitments to streamline review processes to help, local leaders identified a company (Archstar) that designed a pilot tool for assessing compliance with relevant zoning guidelines. The “eCheck” tool is still in beta but was made available to provide the ability to submit a proposed project plan to check against relevant zoning guidelines, and more recently building standards, at the location of the future (re)construction. The purpose of eCheck is to reduce time-intensive feedback loops in permitting processes in which initial plans are submitted and given comments before being amended, resubmitted, and reviewed again for approval. Per information from County and City sources with Archstar, the typical feedback process on an initial submission can take weeks while a pre-check utilizing Archstar will typically be returned in 1-3 days depending on the nature of the proposal (LA County Recovers, 2025). 

A pilot version of eCheck was launched in July but limited to only checking for zoning compliance in R-1 single-family zoned areas within specific regions of Los Angeles County (Altadena, Sunset Mesa, the Palisades) and among those impacted by the fires. In September, the tool was expanded to check submitted plans against essential building code standards. Los Angeles County has also communicated that it intends to expand the tool to include other regions in the County and different types of zoning areas (LA County Recovers, 2025). Los Angeles County maintains a public dashboard that reports on the number of eCheck users and how many projects have been submitted for preliminary review through the tool. Since its launch, 451 users have submitted 201 projects through eCheck (Los Angeles County, 2025). Overall, the County has received 2,504 applications for rebuild projects in areas impacted by the Eaton fire but there is no published data on how many utilized eCheck prior to submission (Los Angeles County, 2025). 

### Data

Zoning or building permit approvals present a clear prediction target for an AI model as they are well-documented and defined through existing local ordinances and guidelines. Zoning guidelines and building standards serve as a clear set of parameters against which to consider plans submitted for initial pre-checks. There is also a large amount of data to ingest, particularly in jurisdictions the size of Los Angeles County or the City of Los Angeles. No data has yet been published on the outcomes of the eCheck tool, while Archstar has described “early adopters” utilizing the tool during its beta phase as helping provide data that will be utilized for testing and validation (Lels, 2025). 

### Fairness

As the pilot program was deployed it was initially limited to Altadena, a historically Black enclave in Los Angeles County. An important equity concern from the fire was the high rate of Black homeowners in Altadena whose homes were within the area impacted by the Eaton fire (61%) relative to their non-Black neighbors (50%), which resulted in Black families experiencing even higher rates of their homes being damaged or destroyed (48%) than their non-Black neighbors (37%) (Ong et al., 2025). Including Altadena in the first phase of deployment of eCheck helps make the resource available to a community traditionally underserved by local government. Prioritization of R-1 single family zoned areas also reflects a focus on families that lost their residence to the fire rather than commercial property owners. Given the local fiscal incentives to attract economic over residential development, this decision reflects an embedded notion of fairness relative to who should first access tools that expedite reconstruction. However it is unclear whether the County or City are taking any actions to ensure that residents are equipped to utilize the tool; without support for less digitally-equipped residents to utilize eCheck, the benefits of its preliminary review may remain solely with those already best situated to navigate permitting processes.  

### Accountability

The nature of eCheck as a tool to help improve submissions before they are officially submitted to and reviewed by City or County staff mitigates many potential accountability concerns. Because eCheck is being presented as a free resource to reduce the challenge in understanding potential issues in zoning or building standard compliance, rather than as a final arbiter in making determinations, there are lower stakes to the model’s output and how staff interact with it. Though outcomes for eCheck remain to be seen, this use of AI may promote an expanded sense of “participatory accountability” as it provides an interaction between street-level bureaucrats and residents at a lower cost to both. Should eCheck prove helpful in addressing the most common initial feedback on planning submissions and reduce the time required of street-level bureaucrats, it may prove to augment the time and discretion available for those same staff in engaging residents on submissions for final review.

### Implementation

The fires struck Los Angeles as the municipal government was already grappling with a $1 billion funding shortfall. The City of Los Angeles Chief Information Officer has described how the fiscal environment “took the oxygen out of the room” during internal deliberations on how to respond to the impacts of the fire with a diminishing workforce and limited operational budget. This fiscal reality prompted greater openness among City leadership and staff to utilize technology to expedite reconstruction (Gordon, 2025). To accelerate rebuilding after the fires, Los Angeles policymakers committed to providing clarity in how projects can be considered on an expedited basis without lengthy review (Office of the California Governor, 2025). This priority suggests a more standardized process, with less potential staff-level discretion, making use of automated decision systems and AI tools complementary to the operating logic of planning departments rather than contrary. However, existing non-fiscal resource deficiencies (e.g., legacy IT systems) and the need to train staff to understand new technology tools have been raised by County officials as limited factors (Gordon, 2025). City leaders reported that departments are already seeing some efficiency benefits from use of eCheck, but acknowledge its low rate of utilization thus far. Whether this enabling environment and the cultural shift it prompted might extend beyond the response to the urgency of reconstruction projects or planning department staff will see it as complementary over the long-term remain open questions. 

# Conclusion & Recommendations 

The framework presented in this paper builds on a body of existing literature that demonstrates how the context for AI deployment, perhaps even more so in local government settings, is inextricably linked to the performance of the model in practice. As local government leaders are presented with promises of AI tools that can make service delivery more effective and efficient, they must pause to consider the nature of the problem and those addressing it before adopting a solution. AI tools can prove useful to how local government prioritizes its resources or provides greater empowerment to its residents, but we suggest our framework and these case studies pose several important recommendations for how to assess such prospects: 

- Carefully investigate how data necessary for any AI tool is collected and structured. Well-defined datasets and clear statutory guidelines can facilitate high-performing prediction models or automated/assistive decision systems, but missing variable problems will undermine their usefulness. 
    
- Be intentional about defining what fairness looks like in addressing the target problem, how it is accounted for in the AI model, and what will be done to structure organizational processes to advance it. Developing or procuring an AI model is a policy exercise in setting priorities for public decisions with impacts on residents and it must be treated as such, rather than a technical or administrative activity. Absent such intentionality, use of an AI model is likely to reproduce existing systemic biases present in its training data and/or its implementing agency. Local governments must consider fairness in both the model’s outcomes and how street-level bureaucrats or the public will interact with them. 

- Consider ways that an AI tool might promote participatory accountability. Local government has an immediate and personal relationship with residents that affords it a high degree of trust, while also delivering tangible services. Some AI tools can enhance resident/government interactions in a manner that is additive and does not undermine the discretion afforded to street-level bureaucrats in making final decisions where they hold expertise or a local government requires accountability. Such opportunities present ripe, low-hanging fruit for local governments seeking to create public value.
    
- Design any procurement processes or contracts with vendors to address accountability. Local governments are likely to turn to external vendors to access AI tools. When doing so, public sector leaders should not fall back on existing template RFPs or contracts for IT projects and consultants. Developing, testing, maintaining, and continuously enhancing an AI tool is a service that requires partnership with a vendor rather than a set of specific and time-limited milestones. The teams managing such vendor relationships should include programmatic, policy, and technical staff.  
    
- Align the tool to the problem, the organizing theory behind the agency addressing it, and the work process of those utilizing it. An AI tool will not be impactful if it does not comport with how the agency is oriented to consider the nature of a problem or carrying out the work of addressing it. External shocks provide an impetus for organizations to reevaluate existing modes of operation, but may not provide lasting change or achieve the public response desired. Street-level bureaucrats hold expertise, as well as the final word in implementation in practice no matter the model’s outcome.
    
There are many areas for further research on the use of AI in local government moving forward. As more local governments procure AI tools, it will be important to analyze publicly available information on RFPs and contracts to assess whether they reflect the level of sophistication necessary for successful partnerships with private-sector vendors. Barring any requirements for how local government discloses its use of AI tools, it will also be relevant for research to track use cases to understand the types of problems being addressed and nature of data being supplied to train models. Finally, researchers partnering with local governments can illuminate where AI tools are helping create public value as examples for replication, while also providing important feedback when they are not.

# References 

Alkhatib, A., & Bernstein, M. (2019). Street-Level Algorithms: A Theory at the Gaps Between Policy and Decisions. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 1–13. [https://doi.org/10.1145/3290605.3300760](https://doi.org/10.1145/3290605.3300760)

Bender, E. M., & Hanna, A. (2025). “The AI Con: How to fight big tech’s hype and create the future we want.” Random House.

Berk, R., Heidari, H., Jabbari, S., Kearns, M., & Roth, A. (2021). “Fairness in Criminal Justice Risk Assessments: The State of the Art.” Sociological Methods & Research, Vol. 50, No. 1, pp. 3–44. [https://doi.org/10.1177/0049124118782533](https://doi.org/10.1177/0049124118782533)

Bosman, E., Park, J., Robinson, M., & Kaiser, R. (2025, June 18). “Software Gains New Status as a Product Under Strict Liability Law.” Morrison Foerester. [https://www.mofo.com/resources/insights/250618-software-gains-new-status-as-a-product-under-strict-liability-law](https://www.mofo.com/resources/insights/250618-software-gains-new-status-as-a-product-under-strict-liability-law)

Burrell, J., & Fourcade, M. (2021). “The Society of Algorithms.” Annual Review Sociology, Vol. 47, pp. 213–237. [https://doi.org/10.1146/annurev-soc-090820-020800](https://doi.org/10.1146/annurev-soc-090820-020800)

Burger Ayogu, K., Hoffnagle, E., Liebman, J., & Mertz, K. (2025) “What is Procurement Excellence?” Partners for Public Good. Retrieved from: [https://partnersforpublicgood.org/procurement-excellence-network/wp-content/uploads/sites/2/2025/03/What-Is-Procurement-Excellence.pdf](https://partnersforpublicgood.org/procurement-excellence-network/wp-content/uploads/sites/2/2025/03/What-Is-Procurement-Excellence.pdf)

Corbett-Davies, S., Pierson, E., Feller, A., & Goel, S. (2016, September 17). “A computer program used for bail and sentencing decisions was labeled biased against blacks. It’s actually not that clear.” The Washington Post. [https://ai.ethicsworkshop.org/Library/LibContentAcademic/NorthpointeProPublicaWAPO.pdf](https://ai.ethicsworkshop.org/Library/LibContentAcademic/NorthpointeProPublicaWAPO.pdf)

Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., & Huq, A. (2017). “Algorithmic Decision Making and the Cost of Fairness.” Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp 797–806. [https://doi.org/10.1145/3097983.3098095](https://doi.org/10.1145/3097983.3098095)

Cook County Treasurer Maria Pappas. (2025, May). “A Broken Property Tax Appeals System.” [https://www.cookcountytreasurer.com/pdfs/appealsreportanalysis/AppealsReportAnalysisEnglish.pdf](https://www.cookcountytreasurer.com/pdfs/appealsreportanalysis/AppealsReportAnalysisEnglish.pdf)

Dobbe, R. I. J. (2022). “System Safety and Artificial Intelligence.” [https://arxiv.org/abs/2202.09292](https://arxiv.org/abs/2202.09292)

Feng, A., & Wu, S. (2019, May 1). “The Myth of the Impartial Machine.” Parametric Press, Issue 1. [https://parametric.press/issue-01/the-myth-of-the-impartial-machine/](https://parametric.press/issue-01/the-myth-of-the-impartial-machine/)

Goldsmith, S. & Yang, J. (2024) “AI and the Transformation of Accountability and Discretion in Urban Governance.” [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4968086](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4968086)

Gordon, K. (2025, August 19). “AI Tech for Good: Approved Building Permits Made Faster After Los Angeles Fires.” Archstar. [https://www.archistar.ai/blog/approved-building-permits/](https://www.archistar.ai/blog/approved-building-permits/)

Government AI Coalition. (n.d.). “Addendum [X]: Requirements for AI Systems.” [https://www.sanjoseca.gov/home/showpublisheddocument/118649/638774761928370000](https://www.sanjoseca.gov/home/showpublisheddocument/118649/638774761928370000)

Government AI Coalition. (n.d.). “AI FactSheet.” [https://www.sanjoseca.gov/home/showpublisheddocument/118633/638774754202870000](https://www.sanjoseca.gov/home/showpublisheddocument/118633/638774754202870000)

Government AI Coalition. (n.d.) “AI Governance Handbook.” .[https://www.sanjoseca.gov/home/showpublisheddocument/118629/638774749167870000](https://www.sanjoseca.gov/home/showpublisheddocument/118629/638774749167870000)

Government AI Coalition & Center for Scientific Evidence in Public Issues. (n.d.) “Key Questions for Government Leaders to Ask When Considering the Use of AI Systems.” [https://www.aaas.org/sites/default/files/2024-11/EPICenterAIQs.pdf?utm_medium=display&utm_source=epi_center-website&utm_campaign=epi2024_aiqs&utm_id=recAwnv42v13cTFZY](https://www.aaas.org/sites/default/files/2024-11/EPICenterAIQs.pdf?utm_medium=display&utm_source=epi_center-website&utm_campaign=epi2024_aiqs&utm_id=recAwnv42v13cTFZY)

Helles, R., & Lomborg, S. (2024). “Techlash or tech change?  How the image of Mark Zuckerberg changed with Cambridge Analytica." Nordicom, University of Gothenburg. [https://doi.org/10.48335/9789188855961-2](https://doi.org/10.48335/9789188855961-2)

Hupe, P., & Hill, M. (2007). “Street-Level Bureaucracy and Public Accountability.” Public Administration, Vol. 85, No. 2, pp. 279–299. [https://doi.org/10.1111/j.1467-9299.2007.00650.x](https://doi.org/10.1111/j.1467-9299.2007.00650.x)

Jones, J. (2025, November 10). “U.S. Trust in Government Depends Upon Party Control.” Gallup. [https://news.gallup.com/poll/697421/trust-government-depends-upon-party-control.aspx](https://news.gallup.com/poll/697421/trust-government-depends-upon-party-control.aspx)

Kleinberg, J., Ludwig, J., Mullainathan, S. & Obermayer, Z. (2015). “Prediction Policy Problems.” American Economic Review: Papers & Proceedings, Vol. 105, No. 5, pp. 491–495. Retrieved from: [https://www.cs.cornell.edu/home/kleinber/aer15-prediction.pdf](https://www.cs.cornell.edu/home/kleinber/aer15-prediction.pdf)

Lels, C. (2025). “Archistar’s eCheck Beta now live in the City and County of Los Angeles.” Archstar. [https://www.archistar.ai/blog/echeck-beta-now-live-in-la/](https://www.archistar.ai/blog/echeck-beta-now-live-in-la/)

LA County Recovers (2025, December 11). “Permitting Progress Dashboard.” [https://recovery.lacounty.gov/rebuilding/permitting-progress-dashboard/](https://recovery.lacounty.gov/rebuilding/permitting-progress-dashboard/)

LA County Recovers (2025). “LA County eCheck for Faster Home Approvals.” [https://recovery.lacounty.gov/la-county-echeck/#:~:text=Visit%20echeck.lacounty.gov%20to,%2Dfor%2DLike%20Rebuild%20Projects.&text=Download%20the%20final%20report%20and,%2Dfor%2DLike%20Rebuild%20Projects](https://recovery.lacounty.gov/la-county-echeck/#:~:text=Visit%20echeck.lacounty.gov%20to,%2Dfor%2DLike%20Rebuild%20Projects.&text=Download%20the%20final%20report%20and,%2Dfor%2DLike%20Rebuild%20Projects).

Los Angeles County (2025, December 12). “LA County eCheck Dashboard.” [https://start.archistar.ai/us/lacounty](https://start.archistar.ai/us/lacounty)

Larson, J., Mattu, S., Kirchner, L., & Angwin, J. (2016, May 23). “How We Analyzed the COMPAS Recidivism Algorithm.” ProPublica. [https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)

Moore, M. (1995). “Creating Public Value: Strategic Management in Government.” Harvard University Press. 

Mulligan, D. K., & Bamberger, K. A. (2019). “Procurement As Policy: Administrative Process for Machine Learning.” Berkeley Technology Law Journal, Vol. 34. [https://doi.org/10.2139/ssrn.3464203](https://doi.org/10.2139/ssrn.3464203)

Nissenbaum, H. (1994). “Computing and Accountability.” Communications of the ACM, Vol. 37, No.1, 72–80. [https://doi.org/10.1145/175222.175228](https://doi.org/10.1145/175222.175228)

Nabatchi, T. & Blomgren Amsler, L. (2014). “Direct Public Engagement in Local Government.” The American Review of Public Administration. [https://digitalcommons.hamline.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1009&context=dri_symposia](https://digitalcommons.hamline.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1009&context=dri_symposia)

Narayanan, A. & Kapoor, S. (2025) “ AI as Normal Technology.” Knight First Amendment Institute at Columbia University. [https://knightcolumbia.org/content/ai-as-normal-technology](https://knightcolumbia.org/content/ai-as-normal-technology)

National League of Cities (2016). “Cities 101: Delegation of Power.” [https://www.nlc.org/resource/cities-101-delegation-of-power/](https://www.nlc.org/resource/cities-101-delegation-of-power/)

National League of Cities (2024, November 13). “AI in Cities - Report & Toolkit.”  [https://www.nlc.org/resource/ai-report-and-toolkit/](https://www.nlc.org/resource/ai-report-and-toolkit/)?

Office of the California Governor (2025, April 30). “Governor Newsom announces launch of new AI tool to supercharge the approval of building permits and speed recovery from Los Angeles Fires.” [https://www.gov.ca.gov/2025/04/30/governor-newsom-announces-launch-of-new-ai-tool-to-supercharge-the-approval-of-building-permits-and-speed-recovery-from-los-angeles-fires/](https://www.gov.ca.gov/2025/04/30/governor-newsom-announces-launch-of-new-ai-tool-to-supercharge-the-approval-of-building-permits-and-speed-recovery-from-los-angeles-fires/)

Ong, P., Pech, C., Frasure, L., Comandur, S., Lee, E., & González, S. (2025, January 28). “LA Wildfires: Impacts on Altadena’s Black Community Rapid Response Data Brief.” University of California, Los Angeles Ralph J. Bunche Center for African American Studies. [https://bunchecenter.ucla.edu/wp-content/uploads/sites/112/2025/02/LA_Wildfire_Altadena_Black_Community_Report.pdf](https://bunchecenter.ucla.edu/wp-content/uploads/sites/112/2025/02/LA_Wildfire_Altadena_Black_Community_Report.pdf)

Ross, R. (2019, April 17) “Why the Cook County Assessor’s Office made its residential assessment code and data public — voluntarily.” Cook County Assessor’s Office. [https://medium.com/@AssessorCook/why-the-cook-county-assessors-office-made-its-residential-assessment-code-and-data-public-c964acfa7b0f](https://medium.com/@AssessorCook/why-the-cook-county-assessors-office-made-its-residential-assessment-code-and-data-public-c964acfa7b0f)

Ross, C. (2022, February 28). “AI gone astray: How subtle shifts in patient data send popular algorithms reeling, undermining patient safety.” Stat10. [https://www.statnews.com/2022/02/28/sepsis-hospital-algorithms-data-shift/](https://www.statnews.com/2022/02/28/sepsis-hospital-algorithms-data-shift/)

Saxena, D & Guha, S. (2024). “Algorithmic Harms in Child Welfare: Uncertainties in Practice, Organization, and Street-level Decision-making.” ACM Journal on Responsible Computing, Vol. 1, No. 1, Article 2.

Sendak, M. (2025, September 2). “Scaling the safe, effective, and ethical use of AI in healthcare.” [https://drive.google.com/file/d/1Hfh_YU26zkGNWVpKnndFZxNaXyQx6iNb/view](https://drive.google.com/file/d/1Hfh_YU26zkGNWVpKnndFZxNaXyQx6iNb/view)

Suresh, H., Tseng, E., Young, M., Gray, M., Pierson, E., & Levy, K. (2024). “Participation in the age of foundation models.” The 2024 ACM Conference on Fairness Accountability and Transparency, 1609–1621. [https://doi.org/10.1145/3630106.3658992](https://doi.org/10.1145/3630106.3658992)

The Lab @ DC. (2025, October) “Can predictive models find homes and businesses at risk of fires?” [https://thelabprojects.dc.gov/fire-risk-prediction](https://thelabprojects.dc.gov/fire-risk-prediction)

U.S. Bureau of Labor Statistics (2025, January 28). “Union Members Summary.” [https://www.bls.gov/news.release/union2.nr0.htm#:~:text=The%20union%20membership%20rate%20continued,educational%20services%20(13.2%20percent)](https://www.bls.gov/news.release/union2.nr0.htm#:~:text=The%20union%20membership%20rate%20continued,educational%20services%20\(13.2%20percent\))

Vaithianathan, R., Putnam-Hornstein, E., & Benavides-Prado, D. “A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions.”

Winner, L. (2020). “Do Artifacts Have Politics? In The whale and the reactor: A search for limits in an age of high technology” Second Edition, pp. 19–39. University of Chicago Press.

Xia, S., Préel-Dumas, C., & Porter, K. (2022, October). “The Value of Predictive Analytics and Machine Learning to Predict Social Service Milestones.” MDRC, Reflections on Methodology. [https://www.mdrc.org/work/publications/value-predictive-analytics-and-machine-learning-predict-social-service-milestones](https://www.mdrc.org/work/publications/value-predictive-analytics-and-machine-learning-predict-social-service-milestones)

Xiang, A., & Raji, I. D. (2019). “On the Legal Compatibility of Fairness Definitions” (Version 1). arXiv. [https://doi.org/10.48550/ARXIV.1912.00761](https://doi.org/10.48550/ARXIV.1912.00761)

Yucer, S., Tektas, F., Al Moubayed, N., & Breckon, T. (2025). “Racial Bias within Face Recognition: A Survey.” ACM Computing Surveys, Vol. 57 No. 4, pp. 1–39. [https://doi.org/10.1145/3705295](https://doi.org/10.1145/3705295)